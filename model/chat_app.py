# ============================================================
# FastAPI + LangChain Agent å¯¹è¯ç³»ç»Ÿ
# ============================================================
# åŠŸèƒ½ï¼šåŸºäº Agent æ¶æ„çš„ AI å¯¹è¯ç³»ç»Ÿï¼Œæ”¯æŒæµå¼å“åº”å’Œå¤šè½®å¯¹è¯
# æŠ€æœ¯æ ˆï¼šFastAPI + LangChain 0.3 + Agent + OpenAI API
# å‚è€ƒï¼š03_study_agent.py çš„ Agent å®ç°æ–¹å¼

import os
import uuid
import yaml
import json
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from collections import defaultdict

from fastapi import FastAPI, HTTPException, status
from fastapi.responses import StreamingResponse, HTMLResponse
from pydantic import BaseModel, Field, ConfigDict

# LangChain 0.3 æ ¸å¿ƒå¯¼å…¥
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnableConfig
from langchain_community.chat_message_histories import ChatMessageHistory


# ============================================================
# 1. åŠ è½½é…ç½®æ–‡ä»¶
# ============================================================

def load_config():
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.getcwd(), "config.yaml")
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


config = load_config()

# ============================================================
# 2. åˆ›å»º FastAPI åº”ç”¨
# ============================================================

app = FastAPI(
    title=config["app"]["name"],
    version=config["app"]["version"],
    description="åŸºäº Agent æ¶æ„çš„ AI å¯¹è¯ç³»ç»Ÿï¼ˆLangChain 0.3ï¼‰"
)

# ============================================================
# 3. ä¼šè¯å†å²ç®¡ç†ï¼ˆä½¿ç”¨ LangChain çš„ ChatMessageHistoryï¼‰
# ============================================================
# è¿™æ˜¯ Agent æ¨èçš„å†å²ç®¡ç†æ–¹å¼ï¼Œä¸ 03_study_agent.py ä¸€è‡´

# å…¨å±€ä¼šè¯å†å²å­˜å‚¨ï¼š{session_id: ChatMessageHistory}
chat_history_store: Dict[str, ChatMessageHistory] = {}

# ä¼šè¯å…ƒæ•°æ®å­˜å‚¨ï¼š{session_id: {"last_active": datetime}}
session_metadata: Dict[str, Dict] = defaultdict(lambda: {
    "last_active": datetime.now()
})


def get_session_history(session_id: str) -> ChatMessageHistory:
    """
    è·å–æˆ–åˆ›å»ºä¼šè¯å†å²
    
    è¿™ä¸ªå‡½æ•°ä¼šè¢« RunnableWithMessageHistory è‡ªåŠ¨è°ƒç”¨
    ç±»ä¼¼äº 03_study_agent.py ä¸­çš„å®ç°æ–¹å¼
    
    Args:
        session_id: ä¼šè¯ ID
    
    Returns:
        ChatMessageHistory å®ä¾‹
    """
    if session_id not in chat_history_store:
        chat_history_store[session_id] = ChatMessageHistory()

    # æ›´æ–°æœ€åæ´»è·ƒæ—¶é—´
    session_metadata[session_id]["last_active"] = datetime.now()

    return chat_history_store[session_id]


def clear_session_history(session_id: str):
    """æ¸…ç©ºæŒ‡å®šä¼šè¯çš„å†å²"""
    if session_id in chat_history_store:
        chat_history_store[session_id].clear()
        del chat_history_store[session_id]
    if session_id in session_metadata:
        del session_metadata[session_id]


def cleanup_expired_sessions() -> int:
    """æ¸…ç†è¿‡æœŸçš„ä¼šè¯"""
    timeout = timedelta(minutes=config["conversation"]["session_timeout"])
    now = datetime.now()

    expired = [
        sid for sid, data in session_metadata.items()
        if now - data["last_active"] > timeout
    ]

    for sid in expired:
        clear_session_history(sid)

    return len(expired)


# ============================================================
# 4. åˆå§‹åŒ– LangChain å¤§æ¨¡å‹
# ============================================================

def get_llm():
    """
    åˆ›å»º LangChain ChatOpenAI å®ä¾‹
    æ”¯æŒ OpenAI åŠå…¼å®¹çš„ APIï¼ˆå¦‚æœ¬åœ°æ¨¡å‹ã€DeepSeekã€æ™ºè°±ç­‰ï¼‰
    """
    llm_config = config["llm"]

    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å– API Key
    api_key = os.getenv("OPENAI_API_KEY") or llm_config.get("api_key")

    # API Key å¯ä»¥ä¸º Noneï¼ˆæŸäº›æœ¬åœ°æ¨¡å‹ä¸éœ€è¦ï¼‰
    return ChatOpenAI(
        model=llm_config["model"],
        temperature=llm_config["temperature"],
        max_tokens=llm_config["max_tokens"],
        api_key=api_key or "not-needed",  # æœ¬åœ°æ¨¡å‹å¯èƒ½ä¸éœ€è¦
        base_url=llm_config.get("base_url"),
        streaming=llm_config["streaming"]
    )


# ============================================================
# 5. åˆ›å»º Agentï¼ˆå‚è€ƒ 03_study_agent.pyï¼‰
# ============================================================

def create_agent_with_history():
    """
    åˆ›å»ºå¸¦å†å²è®°å½•çš„ Agent
    
    æ¶æ„è¯´æ˜ï¼š
    1. ä½¿ç”¨ create_tool_calling_agent åˆ›å»º Agentï¼ˆå³ä½¿æš‚æ—¶æ²¡æœ‰å·¥å…·ï¼‰
    2. ä½¿ç”¨ AgentExecutor æ‰§è¡Œ Agent
    3. ä½¿ç”¨ RunnableWithMessageHistory åŒ…è£…ï¼Œè‡ªåŠ¨ç®¡ç†å†å²
    
    è¿™ç§æ¶æ„çš„ä¼˜åŠ¿ï¼š
    - ç»Ÿä¸€çš„æ¥å£ï¼Œåç»­æ·»åŠ å·¥å…·éå¸¸ç®€å•
    - è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
    - æ”¯æŒæµå¼è¾“å‡º
    
    Returns:
        å¸¦å†å²è®°å½•çš„ Agent Executor
    """

    # 1. å®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼ˆä¸ 03_study_agent.py ç±»ä¼¼ï¼‰
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", config["prompts"]["system"]),
        # å¯¹è¯å†å²å ä½ç¬¦ï¼ˆAgent ä¼šè‡ªåŠ¨å¡«å……ï¼‰
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        # ç”¨æˆ·è¾“å…¥
        ("human", "{input}"),
        # Agent çš„æ€è€ƒè¿‡ç¨‹ï¼ˆå·¥å…·è°ƒç”¨è®°å½•ï¼Œå³ä½¿æ²¡æœ‰å·¥å…·ä¹Ÿéœ€è¦ï¼‰
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # 2. å®šä¹‰å·¥å…·åˆ—è¡¨ï¼ˆç›®å‰ä¸ºç©ºï¼Œåç»­å¯æ‰©å±•ï¼‰
    tools = [get_current_time]

    # 3. åˆ›å»º Agent
    # æ³¨æ„ï¼šå³ä½¿æ²¡æœ‰å·¥å…·ï¼Œä¹Ÿä½¿ç”¨ create_tool_calling_agent
    # è¿™æ ·åç»­æ·»åŠ å·¥å…·æ—¶æ— éœ€ä¿®æ”¹æ¶æ„
    agent = create_tool_calling_agent(
        llm=get_llm(),
        prompt=prompt_template,
        tools=tools,
    )

    # 4. åˆ›å»º AgentExecutor
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=False,  # æ˜¯å¦æ‰“å° Agent æ€è€ƒè¿‡ç¨‹ï¼ˆè°ƒè¯•æ—¶å¯æ”¹ä¸º Trueï¼‰
        handle_parsing_errors=True,  # è‡ªåŠ¨å¤„ç†è§£æé”™è¯¯
        max_iterations=5  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆæ²¡æœ‰å·¥å…·æ—¶é€šå¸¸1æ¬¡å°±å¤Ÿï¼‰
    )

    # 5. åŒ…è£…ä¸ºå¸¦å†å²è®°å½•çš„ Runnable
    # è¿™æ˜¯ LangChain 0.3 æ¨èçš„æ–¹å¼
    agent_history = RunnableWithMessageHistory(
        agent_executor,
        get_session_history,  # å†å²è·å–å‡½æ•°
        input_messages_key="input",  # è¾“å…¥é”®å
        history_messages_key="chat_history"  # å†å²é”®å
    )

    return agent_history


@tool
def get_current_time():
    """è·å–å½“å‰æ—¶é—´"""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# åˆ›å»ºå…¨å±€ Agent å®ä¾‹
agent_with_history = create_agent_with_history()


# ============================================================
# 6. Pydantic æ¨¡å‹å®šä¹‰
# ============================================================

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str = Field(..., min_length=1, description="ç”¨æˆ·æ¶ˆæ¯")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ IDï¼ˆå¯é€‰ï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼‰")

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
                "session_id": "123e4567-e89b-12d3-a456-426614174000"
            }
        }
    )


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    session_id: str = Field(..., description="ä¼šè¯ ID")
    message: str = Field(..., description="AI å›å¤")


class SessionInfo(BaseModel):
    """ä¼šè¯ä¿¡æ¯æ¨¡å‹"""
    session_id: str
    message_count: int
    last_active: str


# ============================================================
# 7. API è·¯ç”±å®šä¹‰
# ============================================================

@app.get("/", response_class=HTMLResponse, tags=["é¡µé¢"])
async def index():
    """è¿”å›å‰ç«¯èŠå¤©é¡µé¢"""
    html_path = os.path.join(os.getcwd(), "chat_ui.html")
    try:
        with open(html_path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return """
        <html>
            <body>
                <h1>èŠå¤©é¡µé¢æœªæ‰¾åˆ°</h1>
                <p>è¯·ç¡®ä¿ chat_ui.html æ–‡ä»¶å­˜åœ¨</p>
            </body>
        </html>
        """


@app.post("/api/chat", response_model=ChatResponse, tags=["å¯¹è¯"])
async def chat(request: ChatRequest):
    """
    æ™®é€šå¯¹è¯æ¥å£ï¼ˆéæµå¼ï¼‰- ä½¿ç”¨ Agent æ¶æ„
    
    - **message**: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
    - **session_id**: ä¼šè¯ IDï¼ˆå¯é€‰ï¼‰
    """
    try:
        # ç”Ÿæˆæˆ–ä½¿ç”¨ä¼šè¯ ID
        session_id = request.session_id or str(uuid.uuid4())

        # è°ƒç”¨ Agentï¼ˆå¸¦å†å²è®°å½•ï¼‰
        # å‚è€ƒ 03_study_agent.py çš„è°ƒç”¨æ–¹å¼
        response = agent_with_history.invoke(
            {"input": request.message},
            config=RunnableConfig(
                configurable={"session_id": session_id}
            )
        )

        # Agent è¿”å›çš„æ˜¯å­—å…¸ï¼ŒåŒ…å« 'output' é”®
        ai_message = response['output']

        return ChatResponse(
            session_id=session_id,
            message=ai_message
        )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¯¹è¯å¤±è´¥: {str(e)}"
        )


@app.post("/api/chat/stream", tags=["å¯¹è¯"])
async def chat_stream(request: ChatRequest):
    """
    æµå¼å¯¹è¯æ¥å£ - çœŸæ­£çš„é€å­—ç¬¦æµå¼å“åº”ï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
    
    å®ç°æ–¹å¼ï¼šä½¿ç”¨ astream_events è·å– LLM å±‚é¢çš„ token çº§åˆ«äº‹ä»¶
    
    - **message**: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
    - **session_id**: ä¼šè¯ IDï¼ˆå¯é€‰ï¼‰
    """
    try:
        # ç”Ÿæˆæˆ–ä½¿ç”¨ä¼šè¯ ID
        session_id = request.session_id or str(uuid.uuid4())

        # æµå¼ç”Ÿæˆå™¨å‡½æ•°
        async def generate():
            """ç”ŸæˆçœŸæ­£çš„æµå¼å“åº”"""
            try:
                full_response = ""

                # é¦–å…ˆå‘é€ä¼šè¯ ID
                yield f"data: {json.dumps({'session_id': session_id, 'type': 'start'}, ensure_ascii=False)}\n\n"

                # ä½¿ç”¨ astream_events è·å–è¯¦ç»†çš„äº‹ä»¶æµ
                # è¿™æ˜¯ LangChain 0.3 æ¨èçš„æµå¼æ–¹å¼ï¼Œå¯ä»¥è·å– token çº§åˆ«çš„äº‹ä»¶
                async for event in agent_with_history.astream_events(
                        {"input": request.message},
                        config=RunnableConfig(
                            configurable={"session_id": session_id}
                        ),
                        version="v2"  # ä½¿ç”¨ v2 ç‰ˆæœ¬çš„äº‹ä»¶æµ
                ):
                    kind = event["event"]

                    # åªå¤„ç† LLM çš„ token äº‹ä»¶ï¼ˆé€å­—ç¬¦è¾“å‡ºï¼‰
                    if kind == "on_chat_model_stream":
                        # ä» LLM è·å–çš„ token
                        chunk = event["data"]["chunk"]
                        if hasattr(chunk, "content") and chunk.content:
                            full_response += chunk.content
                            # ç«‹å³å‘é€ï¼Œå®ç°çœŸæ­£çš„æµå¼æ•ˆæœ
                            yield f"data: {json.dumps({'content': chunk.content}, ensure_ascii=False)}\n\n"

                # å‘é€ç»“æŸæ ‡è®°
                yield f"data: {json.dumps({'type': 'end'}, ensure_ascii=False)}\n\n"

            except Exception as ex:
                error_msg = f"æµå¼ç”Ÿæˆé”™è¯¯: {str(ex)}"
                print(f"é”™è¯¯è¯¦æƒ…: {ex}")
                import traceback
                traceback.print_exc()
                yield f"data: {json.dumps({'type': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"

        # è¿”å›æµå¼å“åº”
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æµå¼å¯¹è¯å¤±è´¥: {str(e)}"
        )


@app.get("/api/history/{session_id}", tags=["å†å²"])
async def get_history(session_id: str):
    """
    è·å–æŒ‡å®šä¼šè¯çš„å¯¹è¯å†å²
    
    - **session_id**: ä¼šè¯ ID
    """
    if session_id not in chat_history_store:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ä¼šè¯ä¸å­˜åœ¨"
        )

    history = chat_history_store[session_id]

    # è½¬æ¢ä¸ºç®€å•çš„å­—å…¸æ ¼å¼
    messages = []
    for msg in history.messages:
        messages.append({
            "role": msg.type,  # 'human', 'ai', 'system'
            "content": msg.content
        })

    return {"session_id": session_id, "messages": messages}


@app.delete("/api/history/{session_id}", tags=["å†å²"])
async def clear_history(session_id: str):
    """
    æ¸…ç©ºæŒ‡å®šä¼šè¯çš„å¯¹è¯å†å²
    
    - **session_id**: ä¼šè¯ ID
    """
    clear_session_history(session_id)
    return {"message": "ä¼šè¯å†å²å·²æ¸…ç©º", "session_id": session_id}


@app.get("/api/sessions", response_model=List[SessionInfo], tags=["ä¼šè¯"])
async def list_sessions():
    """è·å–æ‰€æœ‰æ´»è·ƒçš„ä¼šè¯åˆ—è¡¨"""
    sessions = []
    for sid, history in chat_history_store.items():
        if sid in session_metadata:
            sessions.append(SessionInfo(
                session_id=sid,
                message_count=len(history.messages),
                last_active=session_metadata[sid]["last_active"].strftime("%Y-%m-%d %H:%M:%S")
            ))
    return sessions


@app.post("/api/cleanup", tags=["ç»´æŠ¤"])
async def cleanup_sessions():
    """æ¸…ç†è¿‡æœŸçš„ä¼šè¯"""
    count = cleanup_expired_sessions()
    return {"message": f"å·²æ¸…ç† {count} ä¸ªè¿‡æœŸä¼šè¯"}


@app.get("/api/config", tags=["é…ç½®"])
async def get_config_info():
    """è·å–å½“å‰é…ç½®ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰"""
    return {
        "app": config["app"],
        "llm": {
            "model": config["llm"]["model"],
            "temperature": config["llm"]["temperature"],
            "max_tokens": config["llm"]["max_tokens"],
            "streaming": config["llm"]["streaming"]
        },
        "conversation": config["conversation"],
        "architecture": {
            "framework": "LangChain 0.3",
            "agent_type": "Tool Calling Agent",
            "history_management": "RunnableWithMessageHistory",
            "tools_enabled": True  # åç»­å¯æ·»åŠ å·¥å…·
        }
    }


# ============================================================
# 8. å¯åŠ¨åº”ç”¨
# ============================================================

if __name__ == "__main__":
    import uvicorn

    print("=" * 70)
    print("ğŸš€ å¯åŠ¨ AI å¯¹è¯ç³»ç»Ÿï¼ˆAgent æ¶æ„ - LangChain 0.3ï¼‰")
    print("=" * 70)
    print(f"ğŸ“ åº”ç”¨åç§°: {config['app']['name']}")
    print(f"ğŸ”¢ ç‰ˆæœ¬: {config['app']['version']}")
    print(f"ğŸ¤– æ¨¡å‹: {config['llm']['model']}")
    print(f"ğŸ—ï¸  æ¶æ„: Agent (Tool Calling Agent)")
    print(f"ğŸ“š å†å²ç®¡ç†: RunnableWithMessageHistory")
    print(f"ğŸ”§ å·¥å…·: æš‚æ— ï¼ˆæ¶æ„æ”¯æŒåç»­æ‰©å±•ï¼‰")
    print(f"ğŸŒ è®¿é—®åœ°å€: http://localhost:{config['app']['port']}")
    print(f"ğŸ“– API æ–‡æ¡£: http://localhost:{config['app']['port']}/docs")
    print("=" * 70)

    uvicorn.run(
        app="chat_app:app",
        host=config["app"]["host"],
        port=config["app"]["port"],
        reload=True
    )
