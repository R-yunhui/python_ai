"""
åŸºç¡€ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®ç°

RAG æ ¸å¿ƒæµç¨‹ï¼š
1. æ–‡æ¡£åŠ è½½ï¼ˆDocument Loadingï¼‰- åŠ è½½å„ç§æ ¼å¼çš„æ–‡æ¡£
2. æ–‡æœ¬åˆ‡åˆ†ï¼ˆText Splittingï¼‰- å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå°å—
3. å‘é‡åŒ–ï¼ˆEmbeddingï¼‰- å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
4. å‘é‡å­˜å‚¨ï¼ˆVector Storeï¼‰- å­˜å‚¨å‘é‡åˆ°æ•°æ®åº“
5. æ£€ç´¢ï¼ˆRetrievalï¼‰- æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£
6. ç”Ÿæˆï¼ˆGenerationï¼‰- åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ

å¯¹æ¯” Spring-AI:
Spring-AI ä¹Ÿæœ‰ç±»ä¼¼çš„ RAG æ”¯æŒï¼š
@Autowired
private VectorStore vectorStore;

List<Document> docs = vectorStore.similaritySearch("query");
String prompt = createPromptWithContext(docs, question);
String answer = chatClient.call(prompt);

æœ¬ç¤ºä¾‹å±•ç¤ºæœ€åŸºç¡€çš„ RAG å®ç°ï¼Œä¸ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼Œä»…ç”¨å†…å­˜å­˜å‚¨ã€‚
"""

import os
from typing import List

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS  # ä½¿ç”¨ FAISS ä½œä¸ºå†…å­˜å‘é‡å­˜å‚¨
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
from config import LLMConfig, EmbeddingConfig, TextSplitterConfig, DocumentConfig, RAGConfig


# ========== ç¬¬1æ­¥ï¼šæ–‡æ¡£åŠ è½½ ==========

def load_documents():
    """
    åŠ è½½æ–‡æ¡£
    
    LangChain æä¾›äº†å¤šç§æ–‡æ¡£åŠ è½½å™¨ï¼š
    - TextLoader: åŠ è½½æ–‡æœ¬æ–‡ä»¶
    - PDFLoader: åŠ è½½ PDF æ–‡ä»¶
    - DirectoryLoader: æ‰¹é‡åŠ è½½ç›®å½•ä¸­çš„æ–‡ä»¶
    - CSVLoader: åŠ è½½ CSV æ–‡ä»¶
    - ... è¿˜æœ‰å¾ˆå¤šå…¶ä»–åŠ è½½å™¨
    """
    print("\nğŸ“‚ ç¬¬1æ­¥ï¼šåŠ è½½æ–‡æ¡£")
    print("-" * 60)
    
    # ä½¿ç”¨ DirectoryLoader æ‰¹é‡åŠ è½½ documents ç›®å½•ä¸­çš„æ‰€æœ‰ .txt æ–‡ä»¶
    loader = DirectoryLoader(
        DocumentConfig.DOCUMENTS_DIR,
        glob="*.txt",  # åªåŠ è½½ .txt æ–‡ä»¶
        loader_cls=TextLoader,  # ä½¿ç”¨ TextLoader å¤„ç†æ¯ä¸ªæ–‡ä»¶
        loader_kwargs={'encoding': 'utf-8'}  # æŒ‡å®šç¼–ç 
    )
    
    documents = loader.load()
    
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    for i, doc in enumerate(documents, 1):
        print(f"   {i}. {doc.metadata.get('source', 'Unknown')}")
        print(f"      å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
    
    return documents


# ========== ç¬¬2æ­¥ï¼šæ–‡æœ¬åˆ‡åˆ† ==========

def split_documents(documents):
    """
    åˆ‡åˆ†æ–‡æ¡£
    
    ä¸ºä»€ä¹ˆè¦åˆ‡åˆ†ï¼Ÿ
    1. LLM æœ‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
    2. å°å—æ–‡æœ¬æ›´ç²¾ç¡®ï¼Œæ£€ç´¢æ—¶æ›´å®¹æ˜“æ‰¾åˆ°ç›¸å…³å†…å®¹
    3. æé«˜æ£€ç´¢æ•ˆç‡
    
    RecursiveCharacterTextSplitter ç‰¹ç‚¹ï¼š
    - æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†
    - é€’å½’å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦ï¼ˆæ®µè½ -> å¥å­ -> å•è¯ -> å­—ç¬¦ï¼‰
    - ä¿æŒæ–‡æœ¬çš„è¯­ä¹‰å®Œæ•´æ€§
    """
    print("\nâœ‚ï¸  ç¬¬2æ­¥ï¼šåˆ‡åˆ†æ–‡æ¡£")
    print("-" * 60)
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=TextSplitterConfig.CHUNK_SIZE,  # æ¯å—çš„å¤§å°
        chunk_overlap=TextSplitterConfig.CHUNK_OVERLAP,  # å—ä¹‹é—´çš„é‡å 
        separators=TextSplitterConfig.SEPARATORS,  # åˆ†éš”ç¬¦ä¼˜å…ˆçº§
        length_function=len,  # é•¿åº¦è®¡ç®—å‡½æ•°
    )
    
    chunks = splitter.split_documents(documents)
    
    print(f"âœ… å°† {len(documents)} ä¸ªæ–‡æ¡£åˆ‡åˆ†æˆ {len(chunks)} ä¸ªå—")
    print(f"   å—å¤§å°: {TextSplitterConfig.CHUNK_SIZE} å­—ç¬¦")
    print(f"   å—é‡å : {TextSplitterConfig.CHUNK_OVERLAP} å­—ç¬¦")
    
    # æ˜¾ç¤ºå‰3ä¸ªå—çš„ç¤ºä¾‹
    print("\nğŸ“ å‰3ä¸ªå—çš„ç¤ºä¾‹:")
    for i, chunk in enumerate(chunks[:3], 1):
        preview = chunk.page_content[:100].replace('\n', ' ')
        print(f"   å— {i}: {preview}...")
    
    return chunks


# ========== ç¬¬3æ­¥ï¼šå‘é‡åŒ–å¹¶å­˜å‚¨ ==========

def create_vector_store(chunks):
    """
    åˆ›å»ºå‘é‡å­˜å‚¨
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ Embedding æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    2. å°†å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    
    è¿™é‡Œä½¿ç”¨ FAISSï¼ˆFacebook AI Similarity Searchï¼‰ï¼š
    - å†…å­˜ä¸­çš„å‘é‡å­˜å‚¨
    - å¿«é€Ÿç›¸ä¼¼åº¦æœç´¢
    - é€‚åˆå¼€å‘å’Œå°è§„æ¨¡æ•°æ®
    
    ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ï¼š
    - Milvus: å¼€æºå‘é‡æ•°æ®åº“
    - Pinecone: æ‰˜ç®¡å‘é‡æ•°æ®åº“
    - Weaviate: å¼€æºå‘é‡æœç´¢å¼•æ“
    """
    print("\nğŸ”¢ ç¬¬3æ­¥ï¼šå‘é‡åŒ–å¹¶å­˜å‚¨")
    print("-" * 60)
    
    # åˆå§‹åŒ– Embedding æ¨¡å‹
    # ä½¿ç”¨è‡ªå®šä¹‰ Embeddings ç±»é€‚é…éæ ‡å‡† API æ ¼å¼
    from custom_embeddings import CustomMultimodalEmbeddings
    
    embeddings = CustomMultimodalEmbeddings(
        api_base=EmbeddingConfig.API_BASE,
        api_key=EmbeddingConfig.API_KEY,
        model=EmbeddingConfig.MODEL,
        batch_size=10  # æ¯æ‰¹å¤„ç†10ä¸ªæ–‡æœ¬
    )
    
    print(f"ğŸ“Š ä½¿ç”¨ Embedding æ¨¡å‹: {EmbeddingConfig.MODEL}")
    print(f"   å‘é‡ç»´åº¦: {EmbeddingConfig.DIMENSION}")
    print(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æœ¬å—...")
    
    # ä»æ–‡æ¡£å—åˆ›å»º FAISS å‘é‡å­˜å‚¨
    vector_store = FAISS.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    
    print(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼")
    print(f"   å­˜å‚¨äº† {len(chunks)} ä¸ªå‘é‡")
    
    return vector_store


# ========== ç¬¬4æ­¥ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ ==========

def retrieve_documents(vector_store, query: str, k: int = 4):
    """
    æ£€ç´¢ç›¸å…³æ–‡æ¡£
    
    ç›¸ä¼¼åº¦æœç´¢æ–¹æ³•ï¼š
    1. similarity_search: åŸºæœ¬çš„ç›¸ä¼¼åº¦æœç´¢
    2. similarity_search_with_score: è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
    3. max_marginal_relevance_search: MMR æœç´¢ï¼ˆå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼‰
    """
    print(f"\nğŸ” ç¬¬4æ­¥ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£")
    print("-" * 60)
    print(f"æŸ¥è¯¢: {query}")
    print(f"æ£€ç´¢æ•°é‡: Top {k}")
    
    # ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
    docs_with_scores = vector_store.similarity_search_with_score(query, k=k)
    
    print(f"\nâœ… æ‰¾åˆ° {len(docs_with_scores)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š")
    
    retrieved_docs = []
    for i, (doc, score) in enumerate(docs_with_scores, 1):
        print(f"\n   ğŸ“„ æ–‡æ¡£ {i} (ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}):")
        print(f"      æ¥æº: {doc.metadata.get('source', 'Unknown')}")
        preview = doc.page_content[:150].replace('\n', ' ')
        print(f"      å†…å®¹: {preview}...")
        retrieved_docs.append(doc)
    
    return retrieved_docs


# ========== ç¬¬5æ­¥ï¼šç”Ÿæˆç­”æ¡ˆ ==========

def generate_answer(llm, retrieved_docs, question: str):
    """
    åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
    
    è¿™æ˜¯ RAG çš„æ ¸å¿ƒï¼š
    1. å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
    2. ç»“åˆç”¨æˆ·é—®é¢˜
    3. è®© LLM ç”Ÿæˆç­”æ¡ˆ
    """
    print(f"\nğŸ¤– ç¬¬5æ­¥ï¼šç”Ÿæˆç­”æ¡ˆ")
    print("-" * 60)
    
    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç»„åˆæˆä¸Šä¸‹æ–‡
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    
    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    template = ChatPromptTemplate.from_messages([
        ("system", RAGConfig.SYSTEM_TEMPLATE),
        ("human", RAGConfig.HUMAN_TEMPLATE)
    ])
    
    # åˆ›å»º RAG é“¾
    # è¿™æ˜¯ LangChain çš„ LCELï¼ˆLangChain Expression Languageï¼‰è¯­æ³•
    rag_chain = (
        {"context": lambda x: context, "question": lambda x: x}
        | template
        | llm
        | StrOutputParser()
    )
    
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
    answer = rag_chain.invoke(question)
    
    print(f"\nâœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼")
    return answer


# ========== å®Œæ•´çš„ RAG æµç¨‹ ==========

def basic_rag_demo():
    """å®Œæ•´çš„åŸºç¡€ RAG æ¼”ç¤º"""
    print("=" * 80)
    print("ğŸ¯ åŸºç¡€ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¼”ç¤º")
    print("=" * 80)
    
    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        base_url=LLMConfig.API_BASE,
        model=LLMConfig.MODEL,
        api_key=LLMConfig.API_KEY,
        temperature=LLMConfig.TEMPERATURE,
        max_tokens=LLMConfig.MAX_TOKENS
    )
    
    # ç¬¬1æ­¥ï¼šåŠ è½½æ–‡æ¡£
    documents = load_documents()
    
    # ç¬¬2æ­¥ï¼šåˆ‡åˆ†æ–‡æ¡£
    chunks = split_documents(documents)
    
    # ç¬¬3æ­¥ï¼šå‘é‡åŒ–å¹¶å­˜å‚¨
    vector_store = create_vector_store(chunks)
    
    print("\n" + "=" * 80)
    print("ğŸ“š çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹é—®ç­”äº†")
    print("=" * 80)
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "Python ä¸­çš„åˆ—è¡¨å’Œå…ƒç»„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "LangChain çš„æ ¸å¿ƒæ¦‚å¿µæœ‰å“ªäº›ï¼Ÿ",
        "Milvus æ”¯æŒå“ªäº›ç´¢å¼•ç±»å‹ï¼Ÿ"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'=' * 80}")
        print(f"â“ é—®é¢˜ {i}: {question}")
        print("=" * 80)
        
        # ç¬¬4æ­¥ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = retrieve_documents(vector_store, question, k=RAGConfig.RETRIEVAL_TOP_K)
        
        # ç¬¬5æ­¥ï¼šç”Ÿæˆç­”æ¡ˆ
        answer = generate_answer(llm, retrieved_docs, question)
        
        print(f"\nğŸ’¬ å›ç­”:")
        print("-" * 60)
        print(answer)
        print("-" * 60)
    
    return vector_store


# ========== äº¤äº’å¼é—®ç­” ==========

def interactive_qa(vector_store):
    """äº¤äº’å¼é—®ç­”"""
    print("\n" + "=" * 80)
    print("ğŸ’¬ äº¤äº’å¼é—®ç­”æ¨¡å¼")
    print("=" * 80)
    print("\næç¤º:")
    print("  - è¾“å…¥ä½ çš„é—®é¢˜ï¼Œç³»ç»Ÿä¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹å¹¶ç”Ÿæˆç­”æ¡ˆ")
    print("  - è¾“å…¥ 'exit' é€€å‡º")
    print("\n" + "=" * 80 + "\n")
    
    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        base_url=LLMConfig.API_BASE,
        model=LLMConfig.MODEL,
        api_key=LLMConfig.API_KEY,
        temperature=LLMConfig.TEMPERATURE,
        max_tokens=LLMConfig.MAX_TOKENS
    )
    
    while True:
        try:
            question = input("\nâ“ ä½ çš„é—®é¢˜: ").strip()
            
            if question.lower() in ['exit', 'é€€å‡º', 'quit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if not question:
                continue
            
            # æ£€ç´¢
            retrieved_docs = retrieve_documents(vector_store, question, k=RAGConfig.RETRIEVAL_TOP_K)
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = generate_answer(llm, retrieved_docs, question)
            
            print(f"\nğŸ’¬ å›ç­”:")
            print("-" * 60)
            print(answer)
            print("-" * 60)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å¯¹è¯å·²ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


# ========== ä¸»å‡½æ•° ==========

def main():
    """ä¸»å‡½æ•°"""
    print("\né€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("1. å®Œæ•´ RAG æ¼”ç¤ºï¼ˆè‡ªåŠ¨æµ‹è¯•3ä¸ªé—®é¢˜ï¼‰")
    print("2. äº¤äº’å¼é—®ç­”")
    
    choice = input("\nè¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        vector_store = basic_rag_demo()
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­äº¤äº’å¼é—®ç­”
        continue_qa = input("\næ˜¯å¦ç»§ç»­äº¤äº’å¼é—®ç­”ï¼Ÿ(y/n): ").strip().lower()
        if continue_qa == 'y':
            interactive_qa(vector_store)
    
    elif choice == "2":
        # ç›´æ¥æ„å»ºçŸ¥è¯†åº“å¹¶è¿›å…¥äº¤äº’æ¨¡å¼
        print("\nğŸ”„ æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
        
        llm = ChatOpenAI(
            base_url=LLMConfig.API_BASE,
            model=LLMConfig.MODEL,
            api_key=LLMConfig.API_KEY,
            temperature=LLMConfig.TEMPERATURE
        )
        
        documents = load_documents()
        chunks = split_documents(documents)
        vector_store = create_vector_store(chunks)
        
        print("\nâœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        interactive_qa(vector_store)
    
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œå¯åŠ¨å®Œæ•´æ¼”ç¤º...")
        basic_rag_demo()


if __name__ == "__main__":
    main()

