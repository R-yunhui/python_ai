"""
LangChainå­¦ä¹  - æµå¼è¾“å‡º + å·¥å…·è°ƒç”¨

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£å¦‚ä½•åœ¨æµå¼è¾“å‡ºä¸­å¤„ç†å·¥å…·è°ƒç”¨
2. æŒæ¡ tool_calls çš„æ£€æµ‹å’Œæ‰§è¡Œ
3. å­¦ä¼šå®ç°å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹
4. äº†è§£ Agent å’Œæ‰‹åŠ¨å·¥å…·è°ƒç”¨çš„åŒºåˆ«

é‡è¦ï¼š
æµå¼è¾“å‡º + å·¥å…·è°ƒç”¨éœ€è¦ç‰¹æ®Šå¤„ç†ï¼
ä¸èƒ½ç›´æ¥åœ¨ stream ä¸­è·å– tool_callsï¼Œéœ€è¦ç´¯ç§¯å®Œæ•´å“åº”ã€‚
"""

import os
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from typing import Dict
import json

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å…¨å±€ä¼šè¯å­˜å‚¨
store: Dict[str, ChatMessageHistory] = {}


def get_session_history(session_id: str):
    """è·å–æˆ–åˆ›å»ºä¼šè¯å†å²"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# ============================================================================
# å®šä¹‰å·¥å…·
# ============================================================================

@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´ã€‚å½“ç”¨æˆ·è¯¢é—®ç°åœ¨å‡ ç‚¹ã€å½“å‰æ—¶é—´æ—¶è°ƒç”¨æ­¤å·¥å…·ã€‚"""
    now = datetime.now()
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")


@tool
def get_weather(city: str) -> str:
    """
    è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯ã€‚å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æ—¶ä½¿ç”¨ã€‚
    
    å‚æ•°:
        city: åŸå¸‚åç§°ï¼Œå¦‚"åŒ—äº¬"ã€"ä¸Šæµ·"
    """
    # æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œ15åº¦",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œ20åº¦",
        "å¹¿å·": "é›¨å¤©ï¼Œ28åº¦"
    }
    return weather_data.get(city, f"{city}çš„å¤©æ°”ä¿¡æ¯æš‚æ—¶æ— æ³•è·å–")


@tool
def calculate(expression: str) -> str:
    """
    æ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚å½“ç”¨æˆ·éœ€è¦è®¡ç®—æ—¶ä½¿ç”¨ã€‚
    
    å‚æ•°:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚"2+3*4"
    """
    try:
        result = eval(expression)
        return f"{expression} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


# ============================================================================
# æ–¹æ³•1ï¼šç´¯ç§¯å“åº”åå¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæ¨èï¼‰
# ============================================================================
def example1_accumulate_then_process():
    """
    æ–¹æ³•1ï¼šå…ˆç´¯ç§¯å®Œæ•´çš„æµå¼å“åº”ï¼Œå†å¤„ç†å·¥å…·è°ƒç”¨
    è¿™æ˜¯æœ€ç¨³å®šå¯é çš„æ–¹å¼
    """
    print("\n" + "=" * 60)
    print("æ–¹æ³•1ï¼šç´¯ç§¯å“åº”åå¤„ç†å·¥å…·è°ƒç”¨")
    print("=" * 60)
    
    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])
    
    # åˆå§‹åŒ–æ¨¡å‹å¹¶ç»‘å®šå·¥å…·
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    )
    
    # ç»‘å®šå·¥å…·
    llm_with_tools = llm.bind_tools([get_current_time, get_weather, calculate])
    
    # åˆ›å»ºé“¾
    chain = prompt | llm_with_tools
    
    # åŒ…è£…å†å²è®°å½•
    with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="history"
    )
    
    session_id = "demo_001"
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
        "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "å¸®æˆ‘ç®—ä¸€ä¸‹ 25 * 4 + 100",
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\nã€é—®é¢˜{i}ã€‘{question}")
        print("-" * 60)
        
        # æ­¥éª¤1ï¼šä½¿ç”¨ stream è·å–å“åº”å¹¶ç´¯ç§¯
        response_chunks = []
        print("AIæ€è€ƒä¸­", end="", flush=True)
        
        for chunk in with_history.stream(
            {"question": question},
            config=RunnableConfig(configurable={"session_id": session_id})
        ):
            response_chunks.append(chunk)
            print(".", end="", flush=True)
        
        print()  # æ¢è¡Œ
        
        # æ­¥éª¤2ï¼šä»ç´¯ç§¯çš„chunksä¸­æå–å®Œæ•´çš„AIå“åº”
        ai_message = chunks[0]
        for chunk in chunks[1:]:
            ai_message += chunk
        
        if not ai_message:
            print("âŒ æ²¡æœ‰æ”¶åˆ°å“åº”")
            continue
        
        # æ­¥éª¤3ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls:
            print(f"\nğŸ”§ AIå†³å®šä½¿ç”¨å·¥å…·")
            
            # å¤„ç†æ¯ä¸ªå·¥å…·è°ƒç”¨
            for tool_call in ai_message.tool_calls:
                tool_name = tool_call['name']
                tool_args = tool_call['args']
                tool_id = tool_call['id']
                
                print(f"  å·¥å…·: {tool_name}")
                print(f"  å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}")
                
                # æ­¥éª¤4ï¼šæ‰§è¡Œå·¥å…·
                tool_result = execute_tool(tool_name, tool_args)
                print(f"  ç»“æœ: {tool_result}")
                
                # æ­¥éª¤5ï¼šå°†å·¥å…·ç»“æœæ·»åŠ åˆ°å†å²ï¼Œè®©AIç”Ÿæˆæœ€ç»ˆå›å¤
                history = store[session_id]
                
                # æ·»åŠ AIçš„å·¥å…·è°ƒç”¨æ¶ˆæ¯
                history.add_message(AIMessage(
                    content="",
                    tool_calls=full_response.tool_calls
                ))
                
                # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
                history.add_message(ToolMessage(
                    content=tool_result,
                    tool_call_id=tool_id
                ))
            
            # æ­¥éª¤6ï¼šè®©AIåŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
            print("\nğŸ’¬ AIæœ€ç»ˆå›å¤: ", end="")
            final_response = with_history.stream(
                {"question": ""},  # ç©ºæ¶ˆæ¯ï¼Œè®©AIåŸºäºå·¥å…·ç»“æœå›å¤
                config=RunnableConfig(configurable={"session_id": session_id})
            )
            
            for chunk in final_response:
                if hasattr(chunk, 'content') and chunk.content:
                    print(chunk.content, end="", flush=True)
            print()
            
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æ˜¾ç¤ºå†…å®¹
            print(f"ğŸ’¬ AIå›å¤: {full_response.content}")
        
        print("=" * 60)


# ============================================================================
# æ–¹æ³•2ï¼šæ£€æµ‹æµä¸­çš„å·¥å…·è°ƒç”¨ï¼ˆå¤æ‚ä½†å®æ—¶ï¼‰
# ============================================================================
def example2_detect_in_stream():
    """
    æ–¹æ³•2ï¼šåœ¨æµå¼è¾“å‡ºè¿‡ç¨‹ä¸­æ£€æµ‹å·¥å…·è°ƒç”¨
    æ›´å¤æ‚ï¼Œä½†å¯ä»¥å®æ—¶æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
    """
    print("\n" + "=" * 60)
    print("æ–¹æ³•2ï¼šæµä¸­æ£€æµ‹å·¥å…·è°ƒç”¨")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    )
    
    llm_with_tools = llm.bind_tools([get_current_time, get_weather])
    
    question = "ç°åœ¨å‡ ç‚¹ï¼ŸåŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"
    print(f"\né—®é¢˜: {question}")
    print("-" * 60)
    
    # ç´¯ç§¯å˜é‡
    accumulated_content = ""
    accumulated_tool_calls = []
    
    print("AIå›å¤: ", end="", flush=True)
    
    # æµå¼å¤„ç†
    for chunk in llm_with_tools.stream([HumanMessage(content=question)]):
        # ç´¯ç§¯æ–‡æœ¬å†…å®¹
        if hasattr(chunk, 'content') and chunk.content:
            accumulated_content += chunk.content
            print(chunk.content, end="", flush=True)
        
        # æ£€æµ‹å·¥å…·è°ƒç”¨
        if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
            accumulated_tool_calls.extend(chunk.tool_calls)
    
    print()  # æ¢è¡Œ
    
    # å¤„ç†å·¥å…·è°ƒç”¨
    if accumulated_tool_calls:
        print(f"\nğŸ”§ æ£€æµ‹åˆ° {len(accumulated_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
        
        tool_messages = []
        for tool_call in accumulated_tool_calls:
            tool_name = tool_call['name']
            tool_args = tool_call['args']
            tool_id = tool_call['id']
            
            print(f"\næ‰§è¡Œå·¥å…·: {tool_name}({tool_args})")
            result = execute_tool(tool_name, tool_args)
            print(f"ç»“æœ: {result}")
            
            tool_messages.append(ToolMessage(
                content=result,
                tool_call_id=tool_id
            ))
        
        # å‘é€å·¥å…·ç»“æœï¼Œè·å–æœ€ç»ˆå›å¤
        print("\nğŸ’¬ åŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›å¤: ", end="")
        final_stream = llm.stream([
            HumanMessage(content=question),
            AIMessage(content="", tool_calls=accumulated_tool_calls),
            *tool_messages
        ])
        
        for chunk in final_stream:
            if hasattr(chunk, 'content') and chunk.content:
                print(chunk.content, end="", flush=True)
        print()


# ============================================================================
# æ–¹æ³•3ï¼šä½¿ç”¨ Agent è‡ªåŠ¨å¤„ç†ï¼ˆæœ€ç®€å•ï¼‰
# ============================================================================
def example3_use_agent():
    """
    æ–¹æ³•3ï¼šä½¿ç”¨ Agent è‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨
    è¿™æ˜¯æœ€ç®€å•çš„æ–¹å¼ï¼Œä½†ç‰ºç‰²äº†ä¸€äº›æ§åˆ¶æƒ
    """
    print("\n" + "=" * 60)
    print("æ–¹æ³•3ï¼šä½¿ç”¨ Agent è‡ªåŠ¨å¤„ç†")
    print("=" * 60)
    print("æç¤ºï¼šAgent ä¼šè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨ï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„\n")
    
    from langchain.agents import create_tool_calling_agent, AgentExecutor
    
    # åˆ›å»ºæç¤ºè¯
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"),
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    # åˆ›å»ºæ¨¡å‹
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    )
    
    tools = [get_current_time, get_weather, calculate]
    
    # åˆ›å»º Agent
    agent = create_tool_calling_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
    )
    
    # æµ‹è¯•
    questions = [
        "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
        "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Œé¡ºä¾¿ç®—ä¸€ä¸‹ 15+25",
    ]
    
    for question in questions:
        print(f"\né—®é¢˜: {question}")
        print("=" * 60)
        
        # Agent ä¼šè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨
        result = agent_executor.invoke({"input": question})
        
        print(f"\næœ€ç»ˆç­”æ¡ˆ: {result['output']}")
        print("=" * 60)


# ============================================================================
# å·¥å…·æ‰§è¡Œè¾…åŠ©å‡½æ•°
# ============================================================================
def execute_tool(tool_name: str, tool_args: dict) -> str:
    """
    æ ¹æ®å·¥å…·åç§°å’Œå‚æ•°æ‰§è¡Œç›¸åº”çš„å·¥å…·
    
    å‚æ•°:
        tool_name: å·¥å…·åç§°
        tool_args: å·¥å…·å‚æ•°å­—å…¸
        
    è¿”å›:
        å·¥å…·æ‰§è¡Œç»“æœ
    """
    if tool_name == "get_current_time":
        return get_current_time.invoke({}).content
    elif tool_name == "get_weather":
        return get_weather.invoke(tool_args).content
    elif tool_name == "calculate":
        return calculate.invoke(tool_args).content
    else:
        return f"æœªçŸ¥å·¥å…·: {tool_name}"


# ============================================================================
# å®ç”¨æ¨¡æ¿ï¼šå®Œæ•´çš„èŠå¤©æœºå™¨äººç¤ºä¾‹
# ============================================================================
def example4_complete_chatbot():
    """
    å®ç”¨æ¨¡æ¿ï¼šå®Œæ•´çš„æµå¼è¾“å‡º + å·¥å…·è°ƒç”¨èŠå¤©æœºå™¨äºº
    å¯ä»¥ç›´æ¥ç”¨äºå®é™…é¡¹ç›®
    """
    print("\n" + "=" * 60)
    print("å®Œæ•´ç¤ºä¾‹ï¼šæµå¼èŠå¤©æœºå™¨äºº with å·¥å…·è°ƒç”¨")
    print("=" * 60)
    print("è¾“å…¥ 'exit' é€€å‡ºï¼Œ'history' æŸ¥çœ‹å†å²\n")
    
    # åˆ›å»ºæç¤ºè¯
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚
å¯ç”¨å·¥å…·ï¼š
- get_current_time: è·å–å½“å‰æ—¶é—´
- get_weather: æŸ¥è¯¢åŸå¸‚å¤©æ°”
- calculate: è¿›è¡Œæ•°å­¦è®¡ç®—

æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œå†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ã€‚"""),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])
    
    # åˆå§‹åŒ–æ¨¡å‹
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        temperature=0.7,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    )
    
    llm_with_tools = llm.bind_tools([get_current_time, get_weather, calculate])
    chain = prompt | llm_with_tools
    
    with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="history"
    )
    
    import time
    session_id = f"chat_{int(time.time())}"
    
    while True:
        question = input("\nä½ : ").strip()
        
        if not question:
            continue
        
        if question.lower() == 'exit':
            print("å†è§ï¼")
            break
        
        if question.lower() == 'history':
            if session_id in store:
                history = store[session_id]
                print("\nğŸ“ å¯¹è¯å†å²:")
                for msg in history.messages:
                    role = "ä½ " if msg.type == "human" else "AI"
                    print(f"{role}: {msg.content[:100]}...")
            else:
                print("æš‚æ— å†å²")
            continue
        
        try:
            # ç´¯ç§¯å“åº”
            chunks = []
            print("AI: ", end="", flush=True)
            
            for chunk in with_history.stream(
                {"question": question},
                config=RunnableConfig(configurable={"session_id": session_id})
            ):
                chunks.append(chunk)
                # å®æ—¶æ˜¾ç¤ºéå·¥å…·è°ƒç”¨çš„å†…å®¹
                if hasattr(chunk, 'content') and chunk.content and not hasattr(chunk, 'tool_calls'):
                    print(chunk.content, end="", flush=True)
            
            print()  # æ¢è¡Œ
            
            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            full_response = chunks[-1] if chunks else None
            if full_response and hasattr(full_response, 'tool_calls') and full_response.tool_calls:
                print("ğŸ”§ ä½¿ç”¨å·¥å…·å¤„ç†ä¸­...")
                
                history = store[session_id]
                
                # æ·»åŠ AIæ¶ˆæ¯å’Œå·¥å…·ç»“æœ
                history.add_message(AIMessage(content="", tool_calls=full_response.tool_calls))
                
                for tool_call in full_response.tool_calls:
                    result = execute_tool(tool_call['name'], tool_call['args'])
                    history.add_message(ToolMessage(
                        content=result,
                        tool_call_id=tool_call['id']
                    ))
                
                # è·å–æœ€ç»ˆå›å¤
                print("AI: ", end="", flush=True)
                for chunk in with_history.stream(
                    {"question": ""},
                    config=RunnableConfig(configurable={"session_id": session_id})
                ):
                    if hasattr(chunk, 'content') and chunk.content:
                        print(chunk.content, end="", flush=True)
                print()
        
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    print("ğŸš€ å­¦ä¹ ï¼šæµå¼è¾“å‡º + å·¥å…·è°ƒç”¨")
    print("=" * 60)
    
    try:
        if not os.getenv("OPENAI_API_KEY"):
            print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°OPENAI_API_KEY")
            return
        
        print("""
ä¸‰ç§å¤„ç†æ–¹å¼ï¼š

1. ç´¯ç§¯å“åº”åå¤„ç†ï¼ˆæ¨èï¼‰âœ…
   - æœ€ç¨³å®šå¯é 
   - é€‚åˆå¤§å¤šæ•°åœºæ™¯
   
2. æµä¸­æ£€æµ‹å·¥å…·è°ƒç”¨
   - æ›´å¤æ‚
   - å¯ä»¥å®æ—¶æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
   
3. ä½¿ç”¨ Agent è‡ªåŠ¨å¤„ç†ï¼ˆæœ€ç®€å•ï¼‰âœ…
   - Agent è‡ªåŠ¨å¤„ç†æ‰€æœ‰å·¥å…·è°ƒç”¨
   - ä½†ç‰ºç‰²äº†ä¸€äº›æ§åˆ¶æƒ

é€‰æ‹©è¿è¡Œï¼š
""")
        
        choice = input("é€‰æ‹©ç¤ºä¾‹ (1/2/3/4-å®Œæ•´èŠå¤©æœºå™¨äºº/all): ").strip()
        
        if choice == "1":
            example1_accumulate_then_process()
        elif choice == "2":
            example2_detect_in_stream()
        elif choice == "3":
            example3_use_agent()
        elif choice == "4":
            example4_complete_chatbot()
        elif choice == "all":
            example1_accumulate_then_process()
            input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªç¤ºä¾‹...")
            example2_detect_in_stream()
            input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªç¤ºä¾‹...")
            example3_use_agent()
        
        print("\n" + "=" * 60)
        print("âœ… å­¦ä¹ å®Œæˆï¼")
        print("=" * 60)
        print("""
å…³é”®è¦ç‚¹ï¼š

1. æµå¼è¾“å‡º + å·¥å…·è°ƒç”¨çš„æŒ‘æˆ˜
   - stream() è¿”å›ç”Ÿæˆå™¨ï¼Œæ— æ³•ç›´æ¥è·å– tool_calls
   - éœ€è¦ç´¯ç§¯å®Œæ•´å“åº”æ‰èƒ½æ£€æµ‹å·¥å…·è°ƒç”¨
   
2. å¤„ç†æµç¨‹
   â‘  ç´¯ç§¯ stream çš„æ‰€æœ‰ chunks
   â‘¡ ä»æœ€åä¸€ä¸ª chunk è·å–å®Œæ•´å“åº”
   â‘¢ æ£€æŸ¥ tool_calls
   â‘£ æ‰§è¡Œå·¥å…·
   â‘¤ å°†ç»“æœæ·»åŠ åˆ°å†å²
   â‘¥ è®©AIç”Ÿæˆæœ€ç»ˆå›å¤
   
3. æ¨èæ–¹æ¡ˆ
   - ç®€å•åœºæ™¯ï¼šä½¿ç”¨ Agentï¼ˆç¤ºä¾‹3ï¼‰
   - éœ€è¦æ§åˆ¶ï¼šç´¯ç§¯å“åº”åå¤„ç†ï¼ˆç¤ºä¾‹1ï¼‰
   - ç”Ÿäº§ç¯å¢ƒï¼šå®Œæ•´èŠå¤©æœºå™¨äººï¼ˆç¤ºä¾‹4ï¼‰
   
4. æ³¨æ„äº‹é¡¹
   - å·¥å…·è°ƒç”¨éœ€è¦ä¸¤è½®å¯¹è¯ï¼ˆè°ƒç”¨å·¥å…· + ç”Ÿæˆå›å¤ï¼‰
   - å¿…é¡»å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å†å²è®°å½•
   - ToolMessage éœ€è¦ tool_call_id

ç»§ç»­åŠ æ²¹ï¼ğŸš€
        """)
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

